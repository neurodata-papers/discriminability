---
title: "discrimability"
author: "Greg Kiar"
date: "January 13, 2017"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 11
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
---

# Discriminability in a Nutshell

We are going to generate data from two classes (circles and squares), and demonstrate the
strength of discriminability through evaluating different sampling or processing strategies.
We construct our two class distributions, sample points from them, reconstruct the object,
compute the discriminability of the processing. Discriminability is: computing distance between
all pairs of objects, rank the distances, compute the histogram of same-class ranks, and compute
the mean rank of the distribution.

Before getting started, we will define a few helpful functions and variables which will streamline
this process.

## Setup

### Package Imports

```{r load, warning=FALSE, message=FALSE, results='hide'}
library(abind)
library(rmarkdown)
library(knitr)
library(RColorBrewer)
library(gplots)
sapply(X=dir('../../Code/FlashRupdated/functions/', full.names = TRUE), FUN = source, verbose=FALSE)
graphics.off()
set.seed(123456789)
```

```{r knitr-setup, include=FALSE, results='asis'}
### The following options and figure numbering functions
### were setup by Youngser Park
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE)
dep_auto() # figure out dependencies automatically
opts_chunk$set(cache=FALSE,echo=TRUE,warning=FALSE,message=FALSE,
               comment="#",
               fig.path='./Figures/',
               dpi=227,dev=c('png','pdf'))

opts_knit$set(aliases=c(h='fig.height', w='fig.width', 
                        cap='fig.cap', scap='fig.scap'))
opts_knit$set(eval.after = c('fig.cap','fig.scap'))
knit_hooks$set(document = function(x) {
  gsub('(\\\\end\\{knitrout\\}[\n]+)', '\\1\\\\noindent ', x)
})
#opts_knit$set(animation.fun = hook_scianimator)

knit_hooks$set(plot = function(x, options) {
       paste('<figure><img src="',
             opts_knit$get('base.url'), paste(x, collapse = '.'),
             '"><figcaption>', options$fig.cap, '</figcaption></figure>',
             sep = '')
 })
 
 fn = local({
   i = 0
   function(x) {
     i <<- i + 1
     paste('Figure ', i, ': ', x, sep = '')
   }
 })

 fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste("<b>Figure ", i, ": ", text, "</b><br><br>", sep="")
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
```

### Variables
```{r setup_vars}
lweight <- 4
len <- 500
nsamples <- 9
nsims <- 3
noise1 <- 0.2
tsize <- 4
nsize <- 3
darkcol <- '#28282e'
medcol <- '#98929e'
lightcol <- '#b8b2be'
darkishcol <- '#48424e'
```

### Distribution functions
```{r setup_dists}
circle <- function(){
  theta <- seq(from = 0, to = 2*pi - pi/len, by = 2*pi/len)
  x <- cos(theta)
  y <- sin(theta)
  circ <- list(x, y)
}

square <- function(){
  range <- seq(from = -1, to = 1, by = 2/(len/4))
  static <- rep(1, each = len/4)
  x <- c(range, static, -range, -static) 
  y <- c(static, -range, -static, range)
  sqr <- list(x, y)
}
```

### Plotting functions
```{r setup_plots}
plotshape <- function(xs, ys, typ, cols=medcol){
  size <- lweight
  if (typ == 'p'){
    size <- size/3
  }
  plot(xs, ys, type=typ, axes=FALSE, xlab='', ylab='', pch=20, asp=1, lwd=size, xlim=c(-1, 1), col=darkcol)
  if (typ == 'l'){
    polygon(x=xs, y=ys, col=cols)
  }
}

plotheatmap <- function(data, title=""){
  cols <- colorRampPalette(c("green", "yellow", "red"))(n = 299)
  cols <- c('gray43', cols)
  heatmap.2(data, cellnote=round(data,1), Colv=NA, Rowv=NA, trace="none",
            density.info="none", notecol="black", col=cols, notecex=nsize,
            labRow="", labCol="", key=FALSE, lwid=c(.5,6), lhei=c(.5,6),
            margins=c(4.5,4.5), dendrogram="none")
  par(xpd=NA)
  text(x=0.48, y=1.05, title, cex=nsize)
}

plothistogram <- function(data, title=""){
  layout(1)
  h <- hist(data, breaks=seq(0, 1, l=20), plot=FALSE)
  hist(data, prob=TRUE, col="grey", breaks=seq(0, 1, l=20), ylab="",
       xlab="", main="", xlim=c(0, 1), axes=FALSE)
  d1 <- density(data, bw=0.05, from=0, to=1)
  lines(d1, col="blue", lwd=lweight)
  lines(density(data, bw=0.1, from=0, to=1), lty="dotted", col="darkgreen", lwd=lweight)

  mx <- max(h$count)
  text(x=0, y=-mx/15, "0", cex=nsize-1)
  text(x=1, y=-mx/15, "1", cex=nsize-1)
  text(x=0.5, y=-mx/10, "Normalized Rank", cex=tsize-1)

  mnr <- mean(data)
  score <- paste("MNR:", as.character(round(mnr, 2)))
  lines(c(mnr, mnr), c(0, mx+1), col="red", lwd=lweight)
  text(x=0.5, y=mx+(mx/3), paste(title, score, sep="\n"), cex=nsize)
}

plotseries <- function(x, y, xlab, ylab, fn, title=""){
  layout(1)
  plot(x, y, type="l", xlab=xlab, ylab="", cex.lab=nsize-1, col="blue", axes=FALSE, lwd=lweight)
  lines(x, y, col="black", type="p", lwd=lweight)

  maxy = fn(y)
  lines(c(x[y==maxy], x[y==maxy]), c(0, 1000), col="red", lwd=lweight)

  axis(side=1, at=c(min(x), x[y==maxy], max(x)), lwd=lweight, cex.axis=nsize-1)
  axis(side=2, at=c(min(y), max(y)), lwd=lweight, cex.axis=nsize-1)
  shift <- (range(y)[2] - range(y)[1])*.05
  text(x=(range(x)[2]+range(x)[1])/3, y=max(y)-shift, ylab, cex=tsize)
}
```

### Sampling functions
```{r setup_sampling}
sample2 <- function(data, n){
  pts <- floor(runif(n, min=1, max=len))
  xs <- data[[1]]
  x <- xs[pts] + noise1*runif(n, min=-1, max=1)
  x <- c(x, x[1])
  ys <- data[[2]]
  y <- ys[pts] + noise1*runif(n, min=-1, max=1)
  y <- c(y, y[1])
  samp <- list(x, y)
}

sample1 <- function(data, n){
  pts <- sort(floor(runif(n, min=1, max=len)))
  xs <- data[[1]]
  x <- xs[pts] + noise1*runif(n, min=-1, max=1)
  x <- c(x, x[1])
  ys <- data[[2]]
  y <- ys[pts] + noise1*runif(n, min=-1, max=1)
  y <- c(y, y[1])
  samp <- list(x, y)
}
```

### Utility functions
```{r setup_utils}
distances <- function(data, title="", plot=FALSE){
  diff <- matrix(data=NA, nrow=2*nsims, ncol=2*nsims)
  for (i in 1:(2*nsims)){
    for (j in 1:(2*nsims)){
      diff[i,j] <- norm(data[,,i] - data[,,j], '2')
    }
  }
  if (plot) {
    plotheatmap(diff, title=title)    
  }
  return(diff)
}

simulate_shape <- function(fn, sampling, nsamples, plot=FALSE, cols=lightcol){
  layout(matrix(c(1,2,3, 1,4,5, 1,6,7), 3, 3, byrow = TRUE))
  par(xpd=NA)
  shape <- fn()
  if (plot) {
    plotshape(shape[[1]], shape[[2]], 'l', cols)
    
    arrows(x0=1.2, x1=2.2, y0=1, y1=1.5, length=0.1, lwd = lweight)
    arrows(x0=1.3, x1=2.2, y0=0, y1=0, length=0.1, lwd = lweight)
    arrows(x0=1.2, x1=2.2, y0=-1, y1=-1.5, length=0.1, lwd = lweight)
    text(x=1.5, y=1.5, expression('s'[1]^'a'), cex=tsize)
    text(x=1.7, y=0.3, expression('s'[2]^'a'), cex=tsize)
    text(x=1.5, y=-1.5, expression('s'[3]^'a'), cex=tsize)
  
    arrows(x0=3.5, x1=4.8, y0=1.5, y1=1.5, length=0.1, lwd=lweight)
    arrows(x0=3.5, x1=4.8, y0=0, y1=0, length=0.1, lwd=lweight)
    arrows(x0=3.5, x1=4.8, y0=-1.5, y1=-1.5, length=0.1, lwd=lweight)
  
    text(x=4.15, y=1.8, bquote(p[.(1)]^'a'), cex=tsize)
    text(x=4.15, y=0.3, bquote(p[.(2)]^'a'), cex=tsize)
    text(x=4.15, y=-1.2, bquote(p[.(3)]^'a'), cex=tsize)    
  }

  label <- c()
  data <- array(data=NA, c(2, nsamples, nsims))
  dim(data)
  for (i in 1:nsims){
    samp <- sampling(shape, nsamples)
    if (i <= 3){
      if (plot) {
        plotshape(samp[[1]], samp[[2]], 'p')
        plotshape(samp[[1]], samp[[2]], 'l')        
      }
    }
    data[1, ,i] <- samp[[1]][1:nsamples]
    data[2, ,i] <- samp[[2]][1:nsamples]
    label <- c(label, as.character(substitute(fn)))
  }
  values <- list(label, data)
}
```

## 1-Sample Test-Statistic

### Sampling Shapes

We will start by sampling data from the circle distribution and reconstructing the shapes
from each set of samples.

```{r circle_sampling, fig.align='center'}
vals <- simulate_shape(circle, sample1, nsamples, plot=TRUE, lightcol)
labels <- vals[[1]]
data <- vals[[2]]
```

The same process is then repeated for the square distribution.
```{r square_sampling, fig.align='center'}
vals <- simulate_shape(square, sample1, nsamples, plot=TRUE, darkishcol)
labels <- c(labels, vals[[1]])
data <- abind(data, vals[[2]])
```

### Compute distances
Now we have a data matrix which contains `x` and `y` sampled values, and a list of labels
for each sample identifying whether it belongs to the "circle" or "square" distribution.
We can then compute the distance between observations in each class.
```{r heatmap_1sample, fig.asp=1}
print(labels)
print(dim(data))

diff <- distances(data, title="Distance (Method A)", plot=TRUE)
```

### Rank distances
Next, we can rank this matrix to observe the distances between objects in- and across-
class, and compute the rank distribution function (rdf) of our processed data.
```{r rankmatrix_1sample, fig.asp=1}
rankmatrix <- apply(diff, 1, rank, ties.method="average") - 1
plotheatmap(rankmatrix, "Rank Distance (Method A)")

ranks <- rdf(diff, labels)
plothistogram(ranks, "Rank Distribution Function")
```

## 2-Sample Test Statistic
To use discriminability as a 2-sample test statistic, we simply need to repeat the process
for a different sampling/processing, and compare the two statistics

### Sampling Shapes
Here, to illustrate a potentially worse strategy, we are forgetting to "align" the
samples before constructing the shapes and them, for instance.
```{r sampling_2sample}
vals <- simulate_shape(circle, sample2, nsamples)
labels <- vals[[1]]
data <- vals[[2]]

vals <- simulate_shape(square, sample2, nsamples)
labels <- c(labels, vals[[1]])
data <- abind(data, vals[[2]])
```

### Compute distances
Just as before we compute distances (noticing this is quite a bit worse).
```{r heatmap_2sample, fig.asp=1}
print(labels)
print(dim(data))

diff <- distances(data, title="Distance (Method B)", plot=TRUE)
```

### Rank distances
Ranking and computing the RDF will again illustrate that the method is worse than our
previous method.
```{r rankmatrix_2sample, fig.asp=1}
rankmatrix <- apply(diff, 1, rank, ties.method="average") - 1
plotheatmap(rankmatrix, "Rank Distance (Method B)")

ranks <- rdf(diff, labels)
plothistogram(ranks, "Rank Distribution Function")
```

### Scoring multiple operating points
Performing this analysis as a k-sample test (i.e. score discriminability for multiple apps),
we can pick the operating point that maximizes our discriminability score to be used for
further investigation.
```{r k_sample, fig.asp=1}
samp <- c(  1,  2,    3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14, 15,  16,  17,  18,  19,  20)
mnrs <- c( .2, .31, .36, .47, .53, .58, .67, .78, .84, .92, .96, .97, .97, .99,  1, .99, .98, .97, .94, .89)
plotseries(samp, mnrs, 'N Samples', 'MNR', max)
```

### Performing a task with the optimal pipeline
Using the pipeline that optimizes discriminability for a downstream inference task will also
optimize performance on that task.
```{r inference, fig.asp=1}
mnrs <- c( .2, .31, .36, .47, .53, .58, .67, .78, .84, .92, .96, .97, .97, .99,  1, .99, .98, .97, .94, .89)
loss <- c( 80,  53,  46,  43,  45,  34,  30,  23,  19,  12,   9,   6,   5,   3,  2, 2.5,   4,   7,   8,  11)

sortz = order(mnrs, decreasing = FALSE)
mnrs <- mnrs[sortz]
loss <- loss[sortz]
plotseries(mnrs, loss, 'MNR', 'Prediction\nLoss', min)
```

## Figure Caption

Discriminability as a statistic for optimizing pipelines. When collecting and processing experimental data, a common
objective is to differentiate between different classes of observations, both directly based on the processed data
(i.e. shape) and recovering latent features of the distributions (i.e. colour). In order to evaluate and inform pipeline
development, here taken to include both sampling and preprocessing, we can use discriminability as a 1-sample test. First,
a distance computed between observations; second, the distances are ranked; third, the normalized rank distribution is
computed, and; fourth, the mean of the rank distribution function is computed. Optimal discriminability, a score of 1, is
achieved when of all observations from within a class are more similar to that of another class (i.e. all circles look
more like all other circles than like squares, and vice versa). This process can be repeated for outputs of a different
pipeline, and a 2-sample test can be performed to see which strategy is more discriminable. This can trivially be extended
to a k-sample test, in which we compare k different processing strategies. Optimizing pipelines using discriminability will
then minimize the error on downstream inference (i.e. colour prediction from shape) for any general inference task.
